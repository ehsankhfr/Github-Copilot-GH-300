# Introduction to Prompt Engineering with GitHub Copilot

**Module**: GitHub Copilot Fundamentals Part 1 of 2  
**Duration**: 24 minutes  
**Level**: Beginner to Intermediate  
**Target Audience**: Developers, AI Engineers, Software Engineers

## Overview

GitHub Copilot, powered by OpenAI, is changing the game in software development by accelerating development workflows from initial code creation to production-ready implementations. GitHub Copilot can grasp the intricate details of your project through its training on data containing both natural language and billions of lines of source code from publicly available sources.

To get the most out of GitHub Copilot and maximize your development velocity, you need to know about prompt engineering. Prompt engineering is how you tell GitHub Copilot what you need with precision and efficiency. The quality of the code it gives back, and how quickly you can iterate toward the perfect solution, depends on how clear and strategic your prompts are.

## Learning Objectives

By the end of this module, you'll learn about:

- Prompt engineering principles, best practices, and how GitHub Copilot learns from your prompts
- Advanced prompting strategies including role prompting and chat history management
- The underlying flow of how GitHub Copilot processes user prompts to generate responses
- The data flow for code suggestions and chat in GitHub Copilot
- LLMs (Large Language Models) and their role in GitHub Copilot and prompting
- How to craft effective prompts that optimize GitHub Copilot's performance
- How Copilot handles data from prompts in different situations

---

## 1. Prompt Engineering Foundations and Best Practices

### What is Prompt Engineering?

**Prompt engineering** is the process of crafting clear instructions to guide AI systems, like GitHub Copilot, to generate context-appropriate code tailored to your project's specific needs. This ensures the code is syntactically, functionally, and contextually correct.

### The 4 Ss: Principles of Prompt Engineering

These core rules are the basis for creating effective prompts:

1. **Single**: Always focus your prompt on a single, well-defined task or question. This clarity is crucial for eliciting accurate and useful responses from Copilot.

2. **Specific**: Ensure that your instructions are explicit and detailed. Specificity leads to more applicable and precise code suggestions.

3. **Short**: While being specific, keep prompts concise and to the point. This balance ensures clarity without overloading Copilot or complicating the interaction.

4. **Surround**: Utilize descriptive filenames and keep related files open. This provides Copilot with rich context, leading to more tailored code suggestions.

### Best Practices in Prompt Engineering

#### Provide Enough Clarity

Building on the 'Single' and 'Specific' principles, always aim for explicitness in your prompts.

**Example**: "Write a Python function to filter and return even numbers from a given list"

This prompt is both single-focused and specific.

#### Provide Enough Context with Details

Enrich Copilot's understanding with context, following the 'Surround' principle. The more contextual information provided, the more fitting the generated code suggestions are.

**Tip**: Add comments at the top of your code to give more details about what you want. Use steps to give more detail while keeping it short.

**Note**: Copilot also uses parallel open tabs in your code editor to get more context on the requirements of your code.

#### Provide Examples for Learning

Using examples can clarify your requirements and expectations, illustrating abstract concepts and making the prompts more tangible for Copilot. Well-crafted examples help Copilot understand patterns quickly, leading to more accurate initial suggestions that require fewer revision cycles.

**Effective for**:
- Generating boilerplate code
- Test templates
- Repetitive implementations that form the foundation of larger features

#### Assert and Iterate

One of the keys to unlocking GitHub Copilot's full potential is the practice of strategic iteration. Your first prompt might not always yield production-ready code, and that's perfectly fine.

**Iterative Approach**:
- If the first output isn't quite right, don't start from scratch
- Erase the suggested code
- Enrich your initial comment with added details and examples
- Prompt Copilot again
- Each iteration builds on Copilot's understanding of your specific requirements

---

## 2. How Copilot Learns from Your Prompts

GitHub Copilot operates based on AI models trained on vast amounts of data. To enhance its understanding of specific code contexts, engineers provide it with examples. This led to different training approaches:

### Zero-Shot Learning

GitHub Copilot generates code without any specific examples, relying solely on its foundational training.

**Best for**: Rapidly implementing common patterns and standard functionality

**Example**: Create a function to convert temperatures between Celsius and Fahrenheit by only writing a comment describing what you want. Copilot might generate production-ready code based on its previous training, without any other examples.

### One-Shot Learning

A single example is given, aiding the model in generating more context-aware responses that follow your specific patterns and conventions.

**Best for**: Creating consistent implementations across your codebase, accelerating feature development while maintaining code standards

**Example**: Provide an example of a temperature conversion function, then ask Copilot to create another similar function.

### Few-Shot Learning

Copilot is presented with several examples, which strike a balance between zero-shot unpredictability and the precision of fine-tuning.

**Best for**: Generating sophisticated implementations that handle multiple scenarios and edge cases, reducing time spent on manual testing and refinement

**Example**: Generate code that sends a greeting depending on the time of day, providing multiple time-based examples.

---

## 3. Advanced Prompting Strategies

### Chain Prompting and Managing Chat History

When working on complex features requiring multiple steps, you might engage in extended conversations with GitHub Copilot Chat. While detailed context helps, maintaining long conversation histories can become inefficient.

**Example progression**:
- Turn 1: "Create a user authentication function"
- Turn 2: "Add error handling for invalid credentials"
- Turn 3: "Add unit tests for the authentication function"
- Turn 4: "Add JSDoc comments to document the function"
- Turn 5: "Optimize the function for better performance"

**Note**: Long prompts with full conversation history can consume 2â€“3 PRUs per turn. Summarizing context or resetting the conversation can keep it closer to 1 PRU per request.

**To manage efficiently**:
- Summarize context when conversations become lengthy
- Reset and provide focused context for new features
- Use concise references to previous work instead of repeating full implementations

### Role Prompting for Specialized Tasks

Role prompting involves instructing GitHub Copilot to act as a specific type of expert, which can significantly improve the quality and relevance of generated code for specialized domains.

#### Security Expert Role

**Prompt**: "Act as a cybersecurity expert. Create a password validation function that checks for common vulnerabilities and follows OWASP guidelines."

**Typically generates**:
- Input sanitization
- Protection against common attacks
- Industry standard validation patterns
- Security best practices

#### Performance Optimization Role

**Prompt**: "Act as a performance optimization expert. Refactor this sorting algorithm to handle large datasets efficiently."

**Often results in**:
- Optimized algorithms and data structures
- Memory-efficient implementations
- Scalability considerations
- Performance monitoring suggestions

#### Testing Specialist Role

**Prompt**: "Act as a testing specialist. Create comprehensive unit tests for this payment processing module, including edge cases and error scenarios."

**Typically produces**:
- Thorough test coverage
- Edge case handling
- Mock implementations
- Error condition testing

---

## 4. GitHub Copilot User Prompt Process Flow

### Inbound Flow

#### 1. Secure Prompt Transmission and Context Gathering

The process begins with the secure transmission of the user prompt over HTTPS.

**Copilot collects context details**:
- Code before and after the cursor position
- Filename and type of the file being edited
- Information about adjacent open tabs
- Information on project structure and file paths
- Information on programming languages and frameworks
- Pre-processing using Fill-in-the-Middle (FIM) technique to consider both preceding and following code context

#### 2. Proxy Filter

Once context is gathered and the prompt is built, it passes securely to a proxy server hosted in a GitHub-owned Microsoft Azure tenant. The proxy filters traffic, blocking attempts to hack the prompt or manipulate the system.

#### 3. Toxicity Filtering

Copilot incorporates content filtering mechanisms to ensure generated code and responses don't include or promote:
- Hate speech and inappropriate content
- Personal data (names, addresses, identification numbers)

#### 4. Code Generation with LLM

The filtered and analyzed prompt is passed to LLM Models, which generate appropriate code suggestions based on:
- Copilot's understanding of the prompt
- Surrounding context
- Project-specific requirements

### Outbound Flow

#### 5. Post-Processing and Response Validation

Once the model produces responses, several checks are applied:

**Toxicity filter**: Removes any harmful or offensive generated content

**Code quality checks**:
- Common bugs or vulnerabilities (XSS, SQL injection)
- Ensuring code is robust and secure

**Matching public code (optional)**: Administrators can enable a filter that prevents Copilot from returning suggestions over ~150 characters if they closely resemble existing public code on GitHub

If any part of the response fails these checks, it is either truncated or discarded.

#### 6. Suggestion Delivery and Feedback Loop Initiation

Only responses that pass all filters are delivered to the user. Copilot then initiates a feedback loop based on your actions to:
- Grow its knowledge from accepted suggestions
- Learn and improve through modifications and rejections

#### 7. Repeat for Subsequent Prompts

The process repeats as you provide more prompts, with Copilot continuously:
- Handling user requests
- Understanding their intent
- Generating code in response
- Applying cumulative feedback and interaction data

---

## 5. GitHub Copilot Data Handling

### Data Handling for GitHub Copilot Code Suggestions

- GitHub Copilot in the code editor **does not retain any prompts** like code or other context used for providing suggestions to train foundational models
- It **discards the prompts** once a suggestion is returned
- GitHub Copilot Individual subscribers can **opt-out** of sharing their prompts with GitHub

### Data Handling for GitHub Copilot Chat

GitHub Copilot Chat operates as an interactive platform. Here are key aspects:

**Formatting**:
- Meticulously formats the generated response
- Highlights code snippets for improved readability
- May include options for direct integration into your code

**User Engagement**:
- Ask follow-up questions
- Request clarifications
- Provide additional input
- Chat interface maintains conversation history

**Data Retention**:
- For Copilot Chat used outside the code editor: prompts, suggestions, and supporting context retained for **28 days**
- Same retention applies for CLI, Mobile, and GitHub Copilot Chat on GitHub.com

### Prompt Types Supported by GitHub Copilot Chat

1. **Direct Questions**: "How do I implement a quick sort algorithm in Python?"
2. **Code-Related Requests**: "Write a function to calculate factorial"
3. **Open-Ended Queries**: "What are the best practices for writing clean code?"
4. **Contextual Prompts**: "Here's a part of my code, can you suggest improvements?"

### Limited Context Windows

**Context window** refers to the amount of surrounding code and text the model can process simultaneously.

**GitHub Copilot's context window**:
- Standard Copilot: Approximately 200-500 lines of code or up to a few thousand tokens
- Copilot Chat: 4k tokens (providing broader scope)

**Best practice**: Break down complex problems into smaller, more focused queries or provide relevant code snippets to enhance the model's ability to provide accurate responses.

---

## 6. Large Language Models (LLMs)

### What are LLMs?

Large Language Models (LLMs) are artificial intelligence models designed and trained to understand, generate, and manipulate human language.

**Core Aspects**:

**Volume of Training Data**:
- Exposed to vast amounts of text from diverse sources
- Equipped with broad understanding of language, context, and communication

**Contextual Understanding**:
- Excel in generating contextually relevant and coherent text
- Can complete sentences, paragraphs, or generate whole documents

**Machine Learning and AI Integration**:
- Grounded in machine learning and AI principles
- Neural networks with millions or billions of parameters
- Fine-tuned during training to understand and predict text effectively

**Versatility**:
- Not limited to specific text types or languages
- Can be tailored and fine-tuned for specialized tasks
- Highly versatile and applicable across various domains

### Role of LLMs in GitHub Copilot and Prompting

GitHub Copilot utilizes LLMs to provide context-aware code suggestions. The LLM considers:
- Current file
- Other open files and tabs in the IDE
- Project structure and context

This dynamic approach ensures tailored suggestions, improving productivity.

### Fine-Tuning LLMs

**Fine-tuning** is a critical process that allows us to tailor pretrained large language models for specific tasks or domains.

**Process**:
- Training the model on a smaller, task-specific dataset (target dataset)
- Using knowledge and parameters gained from a large pretrained dataset (source model)

**Purpose**: Adapt LLMs for specific tasks, enhancing their performance

### LoRA Fine-Tuning

**LoRA (Low-Rank Adaptation)** is a clever alternative to traditional full fine-tuning.

**How LoRA Works**:
- Adds smaller trainable parts to each layer of the pretrained model
- Original model remains the same
- Saves time and resources

**Advantages**:
- Beats other adaptation methods like adapters and prefix-tuning
- Gets great results with fewer moving parts
- Works smarter, not harder

**In simple terms**: LoRA fine-tuning makes LLMs better for your specific coding requirements when using Copilot without redoing all the training.

---

## 7. Module Assessment

Choose the best response for each question.

### Question 1
**What's GitHub Copilot?**

- [ ] A. A platform for code repositories
- [ ] B. A model powered by machine learning
- [ ] C. An assistant for coding, powered by OpenAI
- [ ] D. A service for web hosting

### Question 2
**What role does prompting play in utilizing GitHub Copilot effectively?**

- [ ] A. It generates instant bug fixes
- [ ] B. It enhances the quality of code suggestions
- [ ] C. It automates the coding process entirely
- [ ] D. It implements real-time collaboration

### Question 3
**Which of the following rules is a principle of the 4S Method of prompt engineering?**

- [ ] A. Summarize code objectives concisely
- [ ] B. Specify instructions explicitly and in detail
- [ ] C. Streamline processes for efficient code suggestions
- [ ] D. Simplify coding languages for universal understanding

### Question 4
**How does GitHub Copilot handle personal data?**

- [ ] A. It saves all personal data for future references
- [ ] B. It shares personal data with other users for collaborative projects
- [ ] C. It encrypts personal data
- [ ] D. It actively filters out personal data to protect user privacy

### Question 5
**What is LoRA in the context of fine-tuning Large Language Models (LLMs)?**

- [ ] A. A method that adds trainable elements to each layer of the pretrained model without a complete overhaul
- [ ] B. A technology optimizing communication between different coding languages
- [ ] C. A specialized software library enhancing Copilot's performance
- [ ] D. A new programming paradigm supported exclusively by Copilot

### Question 6
**How does Copilot use the context to provide code suggestions?**

- [ ] A. It considers only the prompt text you provide
- [ ] B. It considers the file type but not the content of the file
- [ ] C. It considers the surrounding code, file type, and content of parallel open tabs in the code editor
- [ ] D. It randomly selects context from the internet

### Question 7
**Which of these strategies helps to improve prompt effectiveness in GitHub Copilot?**

- [ ] A. Providing detailed contextual information with clarity
- [ ] B. Making the prompt as general as possible
- [ ] C. Keeping the prompt lengthy and detailed
- [ ] D. Avoiding examples in the prompt to not restrict Copilot's creativity

### Question 8
**What does "Single" mean in the 4 Ss of prompt engineering?**

- [ ] A. Focus your prompt on a single, well-defined task or question
- [ ] B. Use only one programming language
- [ ] C. Provide only one example
- [ ] D. Work on one file at a time

### Question 9
**Which learning approach requires Copilot to generate code without any specific examples?**

- [ ] A. Zero-shot learning
- [ ] B. One-shot learning
- [ ] C. Few-shot learning
- [ ] D. Multi-shot learning

### Question 10
**What is the purpose of the proxy filter in GitHub Copilot's inbound flow?**

- [ ] A. To encrypt all user data
- [ ] B. To block attempts to hack the prompt or manipulate the system
- [ ] C. To generate code suggestions faster
- [ ] D. To store conversation history

### Question 11
**How long does GitHub Copilot Chat retain prompts and suggestions outside the code editor?**

- [ ] A. 7 days
- [ ] B. 14 days
- [ ] C. 28 days
- [ ] D. 90 days

### Question 12
**What is the Fill-in-the-Middle (FIM) technique?**

- [ ] A. A method to complete code in the middle of a function
- [ ] B. Pre-processing that considers both preceding and following code context
- [ ] C. A way to insert comments into existing code
- [ ] D. A debugging technique

### Question 13
**What is the typical context window size for standard GitHub Copilot?**

- [ ] A. 50-100 lines of code
- [ ] B. 200-500 lines of code
- [ ] C. 1000-2000 lines of code
- [ ] D. Unlimited

### Question 14
**Which role prompting would be best for creating comprehensive test suites?**

- [ ] A. Security expert role
- [ ] B. Performance optimization role
- [ ] C. Testing specialist role
- [ ] D. Database expert role

### Question 15
**What happens to prompts in GitHub Copilot code editor after a suggestion is returned?**

- [ ] A. They are stored for future training
- [ ] B. They are shared with other users
- [ ] C. They are discarded
- [ ] D. They are sent to OpenAI servers

### Question 16
**What does the toxicity filter in Copilot's outbound flow do?**

- [ ] A. Speeds up code generation
- [ ] B. Removes any harmful or offensive generated content
- [ ] C. Checks for syntax errors
- [ ] D. Optimizes code performance

### Question 17
**Which of the 4 Ss emphasizes utilizing descriptive filenames and keeping related files open?**

- [ ] A. Single
- [ ] B. Specific
- [ ] C. Short
- [ ] D. Surround

### Question 18
**What is the recommended approach when your first prompt doesn't yield the desired result?**

- [ ] A. Start over with a completely different approach
- [ ] B. Erase the suggested code and enrich your comment with more details
- [ ] C. Accept the suggestion and manually fix it
- [ ] D. Report it as a bug

### Question 19
**How many tokens does Copilot Chat's context window support?**

- [ ] A. 1k tokens
- [ ] B. 2k tokens
- [ ] C. 4k tokens
- [ ] D. 8k tokens

### Question 20
**What is an advantage of LoRA fine-tuning over traditional full fine-tuning?**

- [ ] A. It requires more computing resources
- [ ] B. It changes all parameters of the model
- [ ] C. It saves time and resources by adding smaller trainable parts
- [ ] D. It only works with specific programming languages

### Question 21
**Which type of prompt involves asking specific questions about coding concepts?**

- [ ] A. Direct Questions
- [ ] B. Code-Related Requests
- [ ] C. Open-Ended Queries
- [ ] D. Contextual Prompts

### Question 22
**What percentage can long prompts with full conversation history consume per turn?**

- [ ] A. 0.5-1 PRU
- [ ] B. 2-3 PRUs
- [ ] C. 5-6 PRUs
- [ ] D. 10+ PRUs

### Question 23
**What is the purpose of role prompting?**

- [ ] A. To assign tasks to different developers
- [ ] B. To instruct Copilot to act as a specific type of expert
- [ ] C. To define user permissions
- [ ] D. To organize code into different roles

### Question 24
**Which optional filter prevents Copilot from returning suggestions that closely resemble existing public code?**

- [ ] A. Toxicity filter
- [ ] B. Proxy filter
- [ ] C. Matching public code filter
- [ ] D. Security filter

### Question 25
**What is the main benefit of few-shot learning?**

- [ ] A. It requires no examples
- [ ] B. It only works with one example
- [ ] C. It generates sophisticated implementations handling multiple scenarios and edge cases
- [ ] D. It's faster than zero-shot learning

---

## Answer Key

1. **C** - An assistant for coding, powered by OpenAI
2. **B** - It enhances the quality of code suggestions
3. **B** - Specify instructions explicitly and in detail
4. **D** - It actively filters out personal data to protect user privacy
5. **A** - A method that adds trainable elements to each layer of the pretrained model without a complete overhaul
6. **C** - It considers the surrounding code, file type, and content of parallel open tabs in the code editor
7. **A** - Providing detailed contextual information with clarity
8. **A** - Focus your prompt on a single, well-defined task or question
9. **A** - Zero-shot learning
10. **B** - To block attempts to hack the prompt or manipulate the system
11. **C** - 28 days
12. **B** - Pre-processing that considers both preceding and following code context
13. **B** - 200-500 lines of code
14. **C** - Testing specialist role
15. **C** - They are discarded
16. **B** - Removes any harmful or offensive generated content
17. **D** - Surround
18. **B** - Erase the suggested code and enrich your comment with more details
19. **C** - 4k tokens
20. **C** - It saves time and resources by adding smaller trainable parts
21. **A** - Direct Questions
22. **B** - 2-3 PRUs
23. **B** - To instruct Copilot to act as a specific type of expert
24. **C** - Matching public code filter
25. **C** - It generates sophisticated implementations handling multiple scenarios and edge cases

---

## Summary

In this module, we unveiled the intricacies of optimizing GitHub Copilot through effective prompting. Harnessing the tool's maximum potential lies in the art and science of prompt engineering.

### Key Takeaways:

- **The 4 Ss of Prompt Engineering**: Single, Specific, Short, Surround - fundamental principles for creating effective prompts
- **Learning Approaches**: Zero-shot, one-shot, and few-shot learning help Copilot understand and generate code based on varying levels of examples
- **Advanced Strategies**: Role prompting and chat history management optimize code quality and resource efficiency
- **Process Flow**: Understanding the inbound and outbound flow helps you appreciate Copilot's security and quality measures
- **Data Handling**: Copilot prioritizes privacy by discarding prompts after suggestions and filtering personal data
- **LLMs and Fine-Tuning**: Large Language Models power Copilot, with LoRA fine-tuning enabling efficient customization
- **Context Windows**: Understanding limitations (200-500 lines for standard, 4k tokens for chat) helps you structure better prompts
- **Iterative Approach**: Strategic iteration with enriched prompts accelerates development toward production-ready code

By applying these principles and understanding the underlying technology, you can leverage GitHub Copilot to accelerate your development workflow, produce higher-quality code, and focus on solving bigger problems rather than wrestling with syntax and boilerplate.
